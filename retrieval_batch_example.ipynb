{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch SnowRadar Processing Example\n",
    "A simple workflow using multiple CPUs and landmask/QA filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community imports\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "# pySnowRadar imports|\n",
    "from pySnowRadar import SnowRadar\n",
    "from pySnowRadar.qc import error_check\n",
    "from pySnowRadar.processing import geo_filter,geo_filter_insitu_sites, batch_process\n",
    "from pySnowRadar.algorithms import Wavelet_TN, Peakiness\n",
    "from pySnowRadar.processing import extract_layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "from datetime import datetime, timedelta\n",
    "from thefuzz import process, fuzz\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path, filetype, mode, instrument):\n",
    "    if mode == 'dict':\n",
    "        df_dict = {}\n",
    "        if filetype == 'csv' or filetype=='txt':\n",
    "            if instrument == 'MP':\n",
    "                lon_col = detect_cols(path, keywords=['lon','Longitude'])\n",
    "                lat_col = detect_cols(path, keywords=['lat', 'Latitude'])\n",
    "                time_col = detect_cols(path, keywords=['timestamp', 'datetime'])\n",
    "                var_col = detect_cols(path, keywords=['snow_depth', 'MagnaProbe','depth'])\n",
    "                site_col = detect_cols(path, keywords=['site','site_id'])\n",
    "                type_col = detect_cols(path, keywords=['type','ice_type'])\n",
    "\n",
    "                cols = []\n",
    "                cols.append(lon_col) if lon_col != None else None\n",
    "                cols.append(lat_col) if lat_col != None else None\n",
    "                cols.append(time_col) if time_col != None else None\n",
    "                cols.append(var_col) if var_col != None else None\n",
    "                cols.append(site_col) if site_col != None else None\n",
    "                cols.append(type_col) if type_col != None else None\n",
    "\n",
    "                names = []\n",
    "                names.append('lon') if lon_col != None else None\n",
    "                names.append('lat') if lat_col != None else None\n",
    "                names.append('time') if time_col != None else None\n",
    "                names.append('snow_depth') if var_col != None else None\n",
    "                names.append(\"site_id\") if site_col != None else None\n",
    "                names.append(\"ice_type\") if type_col != None else None\n",
    "\n",
    "                df_import = pd.DataFrame({'names':names}, index=cols)\n",
    "\n",
    "                df_import.sort_index(inplace=True)\n",
    "                df = pd.read_csv(path, skiprows=1,  usecols=list(df_import.index), names=df_import['names'])\n",
    "                if np.mean(df['snow_depth'] > 5): #convert [cm] to [m] in snowdepth\n",
    "                    df['snow_depth'] /= 100\n",
    "                df.loc[df['lon'] < 0, 'lon'] += 360\n",
    "                \n",
    "                df = strtime_to_datetime(df)\n",
    "                \n",
    "                if site_col != None:\n",
    "                    df_dict = split_data(df, df_dict)\n",
    "                    \n",
    "                else:\n",
    "                    df_dict['1'] = df\n",
    "\n",
    "        elif filetype == 'h5':\n",
    "            #This could (should) be more flexible to detect data fields etc.\n",
    "            #Since we only (atm) use 1 h5 file, it is tailored for this file\n",
    "            f = h5py.File(path, 'r')\n",
    "            group = f['eureka_data']\n",
    "            data = group['magnaprobe']\n",
    "            df = pd.DataFrame({'lat':data['latitude'][()], 'lon':data['longitude'][()], 'snow_depth':data['snow_depth'][()], 'site_id':data['site_id'][()]})\n",
    "            df.loc[df['lon'] < 0, 'lon'] += 360\n",
    "            df_dict = split_data(df, df_dict)\n",
    "\n",
    "        return df_dict\n",
    "\n",
    "    elif mode == 'df':\n",
    "        \n",
    "        if filetype == 'csv' or filetype=='txt':\n",
    "            if instrument == 'OIB':\n",
    "                df = pd.read_csv(path, usecols=[0,1,2,7,8,15,16])\n",
    "                df['date'] = df['date'].apply(lambda x: datetime.strptime(str(x), '%Y%m%d'))\n",
    "                df['datetime'] = df['date'] + df['elapsed'].apply(lambda x: timedelta(seconds=x))\n",
    "                df = df[df['snow_depth'] != -99999]\n",
    "                del df['date'], df['elapsed']\n",
    "                \n",
    "            elif instrument == 'ATM':\n",
    "                df = pd.read_csv(path, index_col=0)\n",
    "                \n",
    "        return df\n",
    "\n",
    "def strtime_to_datetime(df):\n",
    "    try:\n",
    "        df['time'] = df['time'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M') )\n",
    "        \n",
    "    except:\n",
    "        df['time'] = clean_time_col(df['time'])\n",
    "        df['time'] = df['time'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M') )\n",
    "\n",
    "    return df \n",
    "\n",
    "def clean_time_col(time): \n",
    "    weird_indices = time.apply(lambda x: len(x) > 16) #length of weird timestamps is higher\n",
    "    time.loc[weird_indices] = np.nan\n",
    "    time = time.ffill()\n",
    "    return time\n",
    " \n",
    "def get_extent_latlon(lon, lat):\n",
    "    '''\n",
    "    Calculates the extent of a given list of latlon coordinates. \n",
    "    Jumps from 360 deg to 0 deg (or visce versa) in longitude are handled.\n",
    "\n",
    "    Inputs:\n",
    "    lon: List of longitude coordinates\n",
    "    lat: List of latitude coordinates\n",
    "\n",
    "    Returns:\n",
    "    extent_latlon: Four point tuple containing the extent: (x0, y0, x1, y2)\n",
    "    '''\n",
    "    \n",
    "    minlat = min(lat)\n",
    "    maxlat = max(lat)\n",
    "    minlon = min(lon)\n",
    "    maxlon = max(lon)\n",
    "\n",
    "    diff_lon = np.diff(lon)\n",
    "\n",
    "    if max(abs(diff_lon)) > 100: #find discontinuity in lon\n",
    "        indices_max,_ = find_peaks(diff_lon, height= 200)\n",
    "        indices_min,_ = find_peaks(diff_lon*-1, height= 200)\n",
    "        indices = sorted(np.append(indices_max, [x for x in indices_min]))\n",
    "        indices.append(len(diff_lon)) \n",
    "\n",
    "        offset = np.ones_like(indices)\n",
    "\n",
    "        if indices[0] in indices_max:\n",
    "            indices.insert(0,0)\n",
    "            offset = np.insert(offset,0,0)\n",
    "\n",
    "        tmp_lon = lon.copy()\n",
    "        for i in range(0,len(indices)-1,2):\n",
    "            ind1 = indices[i] + offset[i]\n",
    "            ind2 = indices[i+1] + 1\n",
    "            tmp_lon[ind1:ind2] = tmp_lon.iloc[ind1:ind2] + 360\n",
    "            \n",
    "        minlon = min(tmp_lon)\n",
    "        if minlon > 360:\n",
    "            minlon -= 360\n",
    "        maxlon = max(tmp_lon)\n",
    "        if maxlon > 360:\n",
    "            maxlon -= 360\n",
    "\n",
    "    extent_latlon = (minlon, minlat, maxlon, maxlat)\n",
    "    return extent_latlon\n",
    "\n",
    "def ordering_latlon(df):\n",
    "    bounds = get_extent_latlon(df['lon'],df['lat'])\n",
    "    idx = df.loc[(df['lat'] == bounds[1])].index[0]\n",
    "\n",
    "    reind = np.arange(0,len(df))\n",
    "    reind = np.delete(reind, idx)\n",
    "    reind = np.insert(reind, 0, idx)\n",
    "    df = df.reindex(reind)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    dists = cdist(list(zip(df['lon'],df['lat'])),list(zip(df['lon'],df['lat'])))\n",
    "    mask = np.zeros(len(df['lon'])).astype(bool)\n",
    "    tst = []\n",
    "    i = 0\n",
    "    for j in range(len(mask)-1):\n",
    "        mask[i] = True\n",
    "        dist_zero = dists[0,:]\n",
    "        dist_zero[mask] = np.nan\n",
    "        row = dists[:,i].copy() + dist_zero\n",
    "        i = np.nanargmin(row)\n",
    "        tst.append(i)\n",
    "    df = df.loc[tst]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "def split_data(df, df_dict):\n",
    "    for uniq in df['site_id'].unique():\n",
    "        df_tmp = df[df['site_id'] == uniq]\n",
    "        if len(df_tmp) > 10000:\n",
    "            df_tmp = ordering_latlon(df_tmp)\n",
    "            df_tmp.dropna(inplace=True)\n",
    "\n",
    "            num_steps = int(np.ceil(len(df_tmp) / 3000))\n",
    "            steps = np.linspace(0,len(df_tmp), num_steps).astype(int)\n",
    "            for i in range(len(steps)-1):\n",
    "                df_tmp_part = df_tmp.loc[steps[i]:steps[i+1]]\n",
    "                df_dict[f'{uniq}.{i}'] = df_tmp_part\n",
    "\n",
    "        else:\n",
    "            df_dict[str(uniq)] = df_tmp\n",
    "\n",
    "    return df_dict   \n",
    "\n",
    "def detect_cols(path, keywords):\n",
    "\n",
    "    df_test = pd.read_csv(path)\n",
    "    cols = list(df_test.columns)\n",
    "    scores = np.zeros(len(cols),dtype='int')\n",
    "    df_scores = pd.DataFrame({'scores':scores},index=cols)\n",
    "    dt = np.dtype(np.str_,np.int_)\n",
    "\n",
    "    for key in keywords:\n",
    "        fuzzy = process.extract(key, cols, limit=len(cols)) \n",
    "        inds = np.array(fuzzy,dtype=dt)[:,0]\n",
    "        scrs = pd.Series(np.array(fuzzy,dtype=dt)[:,1].astype(int), index=inds)\n",
    "        df_scores['scores'] += scrs\n",
    "\n",
    "        if True in (scrs > 80).unique():\n",
    "            tst = df_scores.reset_index()\n",
    "            col_ind = tst[tst['index'] == (scrs > 80).index[0]].index[0]\n",
    "            return col_ind\n",
    "\n",
    "    col_ind = df_scores['scores'].argmax() if df_scores['scores'].max() > len(keywords) * 65 else None\n",
    "    return col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [05:17<00:00, 39.72s/it]\n"
     ]
    }
   ],
   "source": [
    "#WAVELET, 2016\n",
    "\n",
    "sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8'] #'grid1', 'grid2', \n",
    "\n",
    "input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc'\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents/'\n",
    "output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Wavelet/20160419_max_testing'\n",
    "\n",
    "\n",
    "for site in tqdm(sites):\n",
    "\n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        # geo_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(path_to_shapes, site, input_sr_data)\n",
    "\n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 8    \n",
    "        picker = Wavelet_TN # IS CURRENTLY IN LN ISNTEAD OF 10LOG10 MODE!\n",
    "\n",
    "        # FOR WAVELET_TN\n",
    "        params={'snow_density':0.3,\n",
    "                'ref_snow_layer': 1,\n",
    "                'cwt_precision': 10}\n",
    "\n",
    "        res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                        dump_results=True,\n",
    "                        overwrite=True,\n",
    "                        path=os.path.join(output_path, f'{site}')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEAKINESS\n",
    "\n",
    "sites = ['grid1', 'grid2', 'grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8']\n",
    "\n",
    "for site in tqdm(sites):\n",
    "\n",
    "        input_sr_data = glob('/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc')\n",
    "        # geo_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(site, input_sr_data)\n",
    "\n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 8    \n",
    "        picker = Peakiness # IS CURRENTLY IN LN ISNTEAD OF 10LOG10 MODE!\n",
    "        log_peak_threshold = .7\n",
    "        lin_peak_threshold = .5\n",
    "        pp_r_threshold = 30\n",
    "        pp_l_threshold = 25\n",
    "        \n",
    "        # FOR PEAKINESS\n",
    "        params={\n",
    "                'snow_density': 0.3,\n",
    "                'log_peak_threshold' : log_peak_threshold,\n",
    "                'lin_peak_threshold' : lin_peak_threshold, \n",
    "                'pp_r_threshold' : pp_r_threshold, \n",
    "                'pp_l_threshold' : pp_l_threshold\n",
    "        }       \n",
    "\n",
    "        res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                        dump_results=True,\n",
    "                        overwrite=True,\n",
    "                        path=f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/20160419/{site}/'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function Peakiness at 0x300343a60>'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sr_data = glob('/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc')\n",
    "\n",
    "log_peak_threshold = .7\n",
    "lin_peak_threshold = .5\n",
    "pp_r_threshold = 30\n",
    "pp_l_threshold = 25\n",
    "folder_name = str(log_peak_threshold) + '_' + str(lin_peak_threshold) + '_' + str(pp_r_threshold) + '_' + str(pp_l_threshold)\n",
    "outer_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/2016'\n",
    "inner_path = os.path.join(outer_path, folder_name)\n",
    "print(inner_path)\n",
    "Path(inner_path).mkdir(exist_ok=True)\n",
    "\n",
    "params={\n",
    "    'snow_density': 0.3,\n",
    "    'log_peak_threshold' : log_peak_threshold,\n",
    "    'lin_peak_threshold' : lin_peak_threshold, \n",
    "    'pp_r_threshold' : pp_r_threshold, \n",
    "    'pp_l_threshold' : pp_l_threshold\n",
    "}\n",
    "\n",
    "geo_filtered = geo_filter(input_sr_data)\n",
    "# Generate error codes for SR data\n",
    "sr_data = [SnowRadar(sr, 'full') for sr in geo_filtered]\n",
    "error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "\n",
    "workers = 8\n",
    "picker = Peakiness\n",
    "res = batch_process(geo_filtered, picker, params, workers, dump_results=True, overwrite=True, path=inner_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowradar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
